{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhas919/LVQExperiments/blob/main/LVQ_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npg-TgGXRCc7"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o9sZrStt3JmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e9c1f1-1546-418b-9f1f-f27e7771e4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from queue import PriorityQueue\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset = 'NONE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D9inDGyzPYx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b7379d-996c-4e77-e38f-a33740852881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 200)\n",
            "[[-0.23010001  0.07255     0.24698    ... -0.29379001 -0.20446999\n",
            "  -0.23791   ]\n",
            " [-0.10979    -1.07290006  0.69889998 ... -0.21582    -0.48061001\n",
            "  -0.11557   ]\n",
            " [ 0.075252    0.59464002  0.24169999 ... -0.078789    1.01730001\n",
            "  -0.26394001]\n",
            " ...\n",
            " [ 0.51151001 -0.54062003 -0.1815     ...  0.59437001  0.5273\n",
            "  -0.38343   ]\n",
            " [-0.22105999  0.12649     0.40748999 ...  0.31575    -0.17274\n",
            "  -0.75369   ]\n",
            " [ 0.045832   -0.61195999  0.25566    ...  0.41709    -0.48008999\n",
            "  -0.75964999]]\n",
            "(10000, 200)\n",
            "[[-0.14205     0.40746999 -0.37594    ...  0.22135    -0.49772\n",
            "  -0.45609   ]\n",
            " [-0.041663   -0.22078     0.37573001 ... -0.19888    -0.53834999\n",
            "   0.30063   ]\n",
            " [ 0.25580001 -0.11698    -0.32681999 ...  0.37365001 -0.79307997\n",
            "  -0.066078  ]\n",
            " ...\n",
            " [ 0.23751999  0.96261001  0.15023001 ...  0.32071999 -0.39925\n",
            "  -0.46586999]\n",
            " [ 0.40832001 -0.29617     0.70454001 ...  0.81573999 -0.16338\n",
            "   0.65101999]\n",
            " [-0.53675997 -0.77855003  0.71265    ...  0.33496001  0.44966\n",
            "  -0.92355001]]\n"
          ]
        }
      ],
      "source": [
        "# Load GIST CSV\n",
        "# dataset = 'GIST'\n",
        "\n",
        "# Load DBPEDIA Entities Dataset obtained from huggingface\n",
        "# dataset = 'DBPEDIA'\n",
        "\n",
        "# Load GLOVE CSV\n",
        "dataset = 'GLOVE'\n",
        "\n",
        "if dataset == 'GIST':\n",
        "  data_train = np.loadtxt('/content/drive/MyDrive/ScalarQuantization/GIST/gist_train_100K.csv', delimiter=',')\n",
        "  print(np.shape(data_train))\n",
        "  print(data_train)\n",
        "\n",
        "  data_test = np.loadtxt('/content/drive/MyDrive/ScalarQuantization/GIST/gist_test.csv', delimiter=',')\n",
        "  print(np.shape(data_test))\n",
        "  print(data_test)\n",
        "\n",
        "elif dataset == 'DBPEDIA':\n",
        "  df1 = pd.read_parquet('/content/drive/MyDrive/ScalarQuantization/dbpedia-entities-openai3-text-embedding-3-small-1536-100K/data/train-00000-of-00004.parquet')\n",
        "  df2 = pd.read_parquet('/content/drive/MyDrive/ScalarQuantization/dbpedia-entities-openai3-text-embedding-3-small-1536-100K/data/train-00001-of-00004.parquet')\n",
        "  df3 = pd.read_parquet('/content/drive/MyDrive/ScalarQuantization/dbpedia-entities-openai3-text-embedding-3-small-1536-100K/data/train-00002-of-00004.parquet')\n",
        "  df4 = pd.read_parquet('/content/drive/MyDrive/ScalarQuantization/dbpedia-entities-openai3-text-embedding-3-small-1536-100K/data/train-00003-of-00004.parquet')\n",
        "  df = pd.concat([df1, df2, df3, df4], axis=0)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  array = df['text-embedding-3-small-1536-embedding'].to_numpy()\n",
        "  array = np.vstack(array)\n",
        "  print(array.shape)\n",
        "  print(array)\n",
        "\n",
        "elif dataset == 'GLOVE':\n",
        "  data_train = np.loadtxt('/content/drive/MyDrive/ScalarQuantization/Glove/glove200_train_100k.csv', delimiter=',')\n",
        "  print(np.shape(data_train))\n",
        "  print(data_train)\n",
        "\n",
        "  data_test = np.loadtxt('/content/drive/MyDrive/ScalarQuantization/Glove/glove200_test.csv', delimiter=',')\n",
        "  print(np.shape(data_test))\n",
        "  print(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PL3je0ApPYTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d695b6f3-ac5a-4925-cfa2-3bf98dbdf0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Vectors:\n",
            "(100000, 200)\n",
            "[[-0.23010001  0.07255     0.24698    ... -0.29379001 -0.20446999\n",
            "  -0.23791   ]\n",
            " [-0.10979    -1.07290006  0.69889998 ... -0.21582    -0.48061001\n",
            "  -0.11557   ]\n",
            " [ 0.075252    0.59464002  0.24169999 ... -0.078789    1.01730001\n",
            "  -0.26394001]\n",
            " ...\n",
            " [ 0.51151001 -0.54062003 -0.1815     ...  0.59437001  0.5273\n",
            "  -0.38343   ]\n",
            " [-0.22105999  0.12649     0.40748999 ...  0.31575    -0.17274\n",
            "  -0.75369   ]\n",
            " [ 0.045832   -0.61195999  0.25566    ...  0.41709    -0.48008999\n",
            "  -0.75964999]]\n",
            "Query Vectors:\n",
            "(500, 200)\n",
            "[[-0.14205     0.40746999 -0.37594    ...  0.22135    -0.49772\n",
            "  -0.45609   ]\n",
            " [-0.041663   -0.22078     0.37573001 ... -0.19888    -0.53834999\n",
            "   0.30063   ]\n",
            " [ 0.25580001 -0.11698    -0.32681999 ...  0.37365001 -0.79307997\n",
            "  -0.066078  ]\n",
            " ...\n",
            " [-0.46079999 -0.095973   -0.11125    ...  0.071297   -1.12960005\n",
            "   0.50244999]\n",
            " [-0.27873999 -0.22116999  0.70613003 ... -0.34167999 -0.17739999\n",
            "  -0.12989999]\n",
            " [-0.1049     -0.12008     0.35413    ...  0.84759003  0.32635999\n",
            "  -0.71860999]]\n"
          ]
        }
      ],
      "source": [
        "# Setup and Hyperparameters\n",
        "\n",
        "K = 10                 # For top-\"K\"\n",
        "\n",
        "# distfunc = 'EUCLIDEAN'  # distance function: EUCLIDEAN or COSINE\n",
        "distfunc = 'COSINE'\n",
        "\n",
        "# qdtype = 'UB1'          # Quantized data type: UB1 or SB1\n",
        "qdtype = 'SB1'\n",
        "\n",
        "alg = 'PDN'     # Algorithm type: LVQ or PDN\n",
        "\n",
        "num_queries = 500\n",
        "\n",
        "B1 = 8  # Number of bits for quantization\n",
        "res_factors = [1, 10, 100]  # Rescoring factors\n",
        "\n",
        "# Sample Xin\n",
        "# Xin = np.array([\n",
        "#         [1.2, 3.5, 5.1, 7.3],\n",
        "#         [2.1, 4.5, 6.1, 8.3],\n",
        "#         [1.8, 3.0, 4.8, 7.0],\n",
        "#         [2.0, 4.0, 5.0, 8.0],\n",
        "#         [1.5, 3.2, 5.3, 7.8],\n",
        "#         [2.2, 4.3, 6.5, 8.6],\n",
        "#         [1.1, 3.4, 5.6, 7.4],\n",
        "#         [2.5, 4.8, 6.3, 8.9],\n",
        "#         [1.9, 3.9, 5.2, 7.7],\n",
        "#         [2.0, 4.2, 6.0, 8.1],\n",
        "#     ])\n",
        "if dataset == 'GIST':\n",
        "  Xin = data_train\n",
        "  # Xin = data_train[:10000, :]\n",
        "elif dataset == 'DBPEDIA':\n",
        "  Xin = array[:99000,:]\n",
        "  Xin = array[:99000,:]\n",
        "elif dataset == 'GLOVE':\n",
        "  Xin = data_train[:100000,:]\n",
        "print(\"Original Vectors:\")\n",
        "print(np.shape(Xin))\n",
        "print(Xin)\n",
        "\n",
        "assert(K <= np.shape(Xin)[0])\n",
        "\n",
        "if dataset == 'GIST':\n",
        "  Xquery = data_test[:num_queries, :]\n",
        "elif dataset == 'DBPEDIA':\n",
        "  Xquery = array[(100000 - num_queries):, :]\n",
        "elif dataset == 'GLOVE':\n",
        "  Xquery = data_test[:num_queries, :]\n",
        "print(\"Query Vectors:\")\n",
        "print(np.shape(Xquery))\n",
        "print(Xquery)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WpZsLkFJDVE9"
      },
      "outputs": [],
      "source": [
        "class LVQFirstLevel:\n",
        "    distfunc = 'EUCLIDEAN'\n",
        "\n",
        "    def __init__(self, distfunc):\n",
        "      self.distfunc = distfunc\n",
        "\n",
        "    ## LVQ_first_level\n",
        "\n",
        "    # \"Quantized Data Type\"\n",
        "    # qdtype: 'UB1' -> {0...255}\n",
        "    # qdtype: 'SB1' -> {-128...127}\n",
        "\n",
        "    # \"Algorithm\"\n",
        "    # alg: 'LVQ' -> LVQ implementation, Per Vector Norm.\n",
        "    #       l, u and delta are computed along the vector for each vector\n",
        "    # alg: 'PDN' -> Per Dimension Normalization.\n",
        "    #       l, u and delta are computed across vectors for each dimension\n",
        "    # alg: 'GMM' -> Global Min Max Algorithm\n",
        "    #       l, u and delta are computed across vectors and across all dimensions\n",
        "\n",
        "    # isQuery: False indicates these are graph vectors, True indicates these are query vectors\n",
        "    @staticmethod\n",
        "    def LVQ_first_level(Xin, B1, alg = 'LVQ', isQuery = False, qdtype = 'UB1', X_means_p = None, l_p = None, delta_p = None):\n",
        "        if (alg not in ['LVQ', 'PDN', 'GMM']):\n",
        "          assert False, \"Unrecognized algorithm parameter\"\n",
        "        if (qdtype not in ['UB1', 'SB1']):\n",
        "          assert False, \"Unrecognized quantization data type parameter\"\n",
        "\n",
        "        if(isQuery):\n",
        "          if(alg == 'LVQ'):\n",
        "            assert(X_means_p is not None)\n",
        "          elif(alg == 'PDN' or alg == 'GMM'):\n",
        "            assert(l_p is not None)\n",
        "            assert(delta_p is not None)\n",
        "          # Todo: Check shapes of arguments\n",
        "\n",
        "        print(\"Xin shape:\", np.shape(Xin))\n",
        "        num_rows, num_cols = Xin.shape\n",
        "\n",
        "        # Compute the mean of each dimension, if required\n",
        "        if(alg == 'LVQ'):\n",
        "          if(not isQuery):\n",
        "            X_means = np.mean(Xin, axis=0)\n",
        "          else:\n",
        "            X_means = X_means_p\n",
        "          print(\"X_means shape:\", np.shape(X_means))\n",
        "        elif(alg == 'PDN' or alg == 'GMM'):\n",
        "          X_means = None\n",
        "\n",
        "        # Assign to X\n",
        "        if(alg == 'LVQ'):\n",
        "          # Center the input vectors\n",
        "          X = Xin - X_means\n",
        "        elif(alg == 'PDN' or alg == 'GMM'):\n",
        "          X = Xin\n",
        "        print(\"X shape:\", np.shape(X))\n",
        "\n",
        "        # Compute l, u\n",
        "        if(alg == 'LVQ'): # LVQ: Always recompute l, u\n",
        "          # Compute per-vector max and min across dimensions\n",
        "          u = np.max(X, axis=1)\n",
        "          l = np.min(X, axis=1)\n",
        "        elif(alg == 'PDN'):\n",
        "          if(not isQuery):\n",
        "            # Compute per-dimension max and min across vectors\n",
        "            u = np.max(X, axis=0)\n",
        "            l = np.min(X, axis=0)\n",
        "          else:\n",
        "            # Reuse l\n",
        "            l = l_p\n",
        "            u = None\n",
        "        elif(alg == 'GMM'):\n",
        "          if(not isQuery):\n",
        "            # Compute global max and min\n",
        "            u = np.max(X)\n",
        "            l = np.min(X)\n",
        "          else:\n",
        "            # Reuse l\n",
        "            l = l_p\n",
        "            u = None\n",
        "        print(\"l shape:\", np.shape(l))\n",
        "\n",
        "        # Compute the quantization delta and its inverse\n",
        "        if(alg == 'LVQ'): # Always recompute delta\n",
        "          delta = (u - l) / (2**B1 - 1)\n",
        "        elif(alg == 'PDN' or alg == 'GMM'):\n",
        "          if(not isQuery):\n",
        "            delta = (u - l) / (2**B1 - 1)\n",
        "          else:   # Reuse delta_p\n",
        "            delta = delta_p\n",
        "\n",
        "        if(alg == 'LVQ' or alg == 'PDN'):\n",
        "          delta[delta == 0] = 1.0  # Avoid division by zero\n",
        "        elif(alg == 'GMM' and delta == 0):\n",
        "          delta = 1   # Avoid division by zero\n",
        "\n",
        "        inv_delta = 1.0 / delta\n",
        "        print(\"delta shape:\", np.shape(delta))\n",
        "        print(\"inv_delta shape:\", np.shape(inv_delta))\n",
        "\n",
        "        # Compute the quantized codes\n",
        "        if(alg == 'LVQ'):\n",
        "          tcodes = np.floor(inv_delta[:, None] * (X - l[:, None]) + 0.5)\n",
        "        elif(alg == 'PDN' or alg == 'GMM'):\n",
        "          tcodes = np.floor(inv_delta[None, :] * (X - l[None, :]) + 0.5)\n",
        "        tcodes = np.clip(tcodes, 0, 2**B1 - 1)\n",
        "\n",
        "        # Convert the codes into integer format\n",
        "        codes = tcodes.astype(int)\n",
        "\n",
        "        # Shift to {-128...127} if required\n",
        "        if(qdtype == 'SB1'):\n",
        "          codes -= 128\n",
        "\n",
        "        # Compute the first-level reconstruction \"Xcomp\"\n",
        "        if(qdtype == 'UB1'):\n",
        "          temp_codes = codes\n",
        "        elif(qdtype == 'SB1'):\n",
        "          temp_codes = codes+128\n",
        "\n",
        "        if(alg == 'LVQ'):\n",
        "          Xcomp = delta[:, None] * codes + l[:, None]\n",
        "        elif(alg == 'PDN' or alg == 'GMM'):\n",
        "          Xcomp = delta[None, :] * codes + l[None, :]\n",
        "\n",
        "        # Add back the mean, if you had de-meaned\n",
        "        if(alg == 'LVQ'):\n",
        "          Xcomp += X_means\n",
        "\n",
        "        print(\"Xcomp shape:\", np.shape(Xcomp))\n",
        "        print(\"codes shape:\", np.shape(codes))\n",
        "\n",
        "        return Xcomp, delta, codes, l, X_means\n",
        "\n",
        "    @staticmethod\n",
        "    def euclidean_distance(vector1, vector2):\n",
        "        if len(vector1) != len(vector2):\n",
        "            raise ValueError(\"Vectors must have the same length.\")\n",
        "        return np.sqrt(np.sum((np.array(vector1) - np.array(vector2)) ** 2))\n",
        "\n",
        "    @staticmethod\n",
        "    def cosine_distance(vector1, vector2):\n",
        "        \"\"\"Computes the cosine distance between two vectors.\"\"\"\n",
        "        dot_product = np.dot(vector1, vector2)\n",
        "        norm_vec1 = np.linalg.norm(vector1)\n",
        "        norm_vec2 = np.linalg.norm(vector2)\n",
        "\n",
        "        if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "            return 1.0  # If either vector is zero, return max distance\n",
        "\n",
        "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "        return 1 - cosine_similarity  # Convert similarity to distance\n",
        "\n",
        "    def distance(self, vector1, vector2):\n",
        "      if(self.distfunc == 'EUCLIDEAN'):\n",
        "        return self.euclidean_distance(vector1, vector2)\n",
        "      elif(self.distfunc == 'COSINE'):\n",
        "        return self.cosine_distance(vector1, vector2)\n",
        "\n",
        "    # Compute reconstruction error\n",
        "    @staticmethod\n",
        "    def recon_error(original, reconstructed):\n",
        "      if original.shape != reconstructed.shape:\n",
        "        raise ValueError(\"Original and reconstructed vectors must have the same shape.\")\n",
        "      return np.mean(np.sqrt(np.sum((original - reconstructed) ** 2, axis=1)))\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_accuracy(exact_knn, approx_knn):\n",
        "      # if len(exact_knn) != len(approx_knn):\n",
        "      #   raise ValueError(\"exact_knn and approx_knn must have the same length.\")\n",
        "\n",
        "      # exact_knn is of length K, approx_knn could be of length K*rescore_factor\n",
        "      K = len(exact_knn)\n",
        "      common = len(set(exact_knn).intersection(approx_knn))\n",
        "\n",
        "      # Compute the accuracy as the fraction of correct neighbors\n",
        "      accuracy = common / K\n",
        "      return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrbmBuhrIVlU"
      },
      "source": [
        "# LVQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_wy8Ee_hM8l",
        "outputId": "56377252-40af-4154-e85b-547a795c3557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xin shape: (100000, 200)\n",
            "X_means shape: (200,)\n",
            "X shape: (100000, 200)\n",
            "l shape: (100000,)\n",
            "delta shape: (100000,)\n",
            "inv_delta shape: (100000,)\n",
            "Xcomp shape: (100000, 200)\n",
            "codes shape: (100000, 200)\n",
            "Original Vectors:\n",
            "[[-0.23010001  0.07255     0.24698    ... -0.29379001 -0.20446999\n",
            "  -0.23791   ]\n",
            " [-0.10979    -1.07290006  0.69889998 ... -0.21582    -0.48061001\n",
            "  -0.11557   ]\n",
            " [ 0.075252    0.59464002  0.24169999 ... -0.078789    1.01730001\n",
            "  -0.26394001]\n",
            " ...\n",
            " [ 0.51151001 -0.54062003 -0.1815     ...  0.59437001  0.5273\n",
            "  -0.38343   ]\n",
            " [-0.22105999  0.12649     0.40748999 ...  0.31575    -0.17274\n",
            "  -0.75369   ]\n",
            " [ 0.045832   -0.61195999  0.25566    ...  0.41709    -0.48008999\n",
            "  -0.75964999]]\n",
            "\n",
            "8-Bit Quantized Vectors:\n",
            "[[-26  15  44 ... -77 -52   2]\n",
            " [ 26 -64  90 ...  -4 -22  40]\n",
            " [-10  31  -2 ... -45  59 -25]\n",
            " ...\n",
            " [ 47 -60 -26 ...  33  32 -22]\n",
            " [ -8  25  57 ...  28 -23 -51]\n",
            " [ 22 -61  37 ...  37 -58 -51]]\n",
            "\n",
            "After unquantizing from 8-bit format to float:\n",
            "[[-0.9729153  -0.66917285 -0.4930426  ... -1.03484139 -0.9461347\n",
            "  -0.97858262]\n",
            " [-1.57473331 -2.53749534 -0.76775478 ... -1.68479306 -1.94655912\n",
            "  -1.58223122]\n",
            " [-1.35517073 -0.83058753 -1.19081755 ... -1.51307481 -0.40724763\n",
            "  -1.69034712]\n",
            " ...\n",
            " [-0.83146498 -1.88453531 -1.52036008 ... -0.74490361 -0.81126207\n",
            "  -1.72111376]\n",
            " [-1.30641435 -0.95958444 -0.67945018 ... -0.76764569 -1.2566274\n",
            "  -1.8391869 ]\n",
            " [-1.07714251 -1.74156303 -0.87004095 ... -0.71197106 -1.60457197\n",
            "  -1.88770591]]\n",
            "Xin shape: (500, 200)\n",
            "X_means shape: (200,)\n",
            "X shape: (500, 200)\n",
            "l shape: (500,)\n",
            "delta shape: (500,)\n",
            "inv_delta shape: (500,)\n",
            "Xcomp shape: (500, 200)\n",
            "codes shape: (500, 200)\n",
            "\n",
            "================ j =  0\n",
            "\n",
            "Average Quantized Accuracy for res_factor= [1, 10, 100] :   nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: [0.5, 0.8, 1.0] \n",
            "\n",
            "Accuracy: [0.4, 0.7, 1.0] \n",
            "\n",
            "Accuracy: [0.0, 0.0, 0.1] \n",
            "\n",
            "Accuracy: [0.5, 0.9, 1.0] \n",
            "\n",
            "Accuracy: [0.0, 0.0, 0.0] \n",
            "\n",
            "Accuracy: [0.8, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.5, 0.6, 0.8] \n",
            "\n",
            "Accuracy: [0.3, 0.8, 1.0] \n",
            "\n",
            "Accuracy: [0.0, 0.0, 0.1] \n",
            "\n",
            "Accuracy: [0.5, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.7, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.3, 0.7, 0.9] \n",
            "\n",
            "Accuracy: [0.3, 0.6, 1.0] \n",
            "\n",
            "Accuracy: [0.5, 0.9, 1.0] \n",
            "\n",
            "Accuracy: [0.6, 0.9, 0.9] \n",
            "\n",
            "Accuracy: [0.2, 0.5, 0.7] \n",
            "\n",
            "Accuracy: [0.7, 0.9, 1.0] \n",
            "\n",
            "Accuracy: [0.7, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.3, 0.9, 0.9] \n",
            "\n",
            "Accuracy: [0.5, 0.9, 1.0] \n",
            "\n",
            "Accuracy: [0.3, 0.7, 0.9] \n",
            "\n",
            "Accuracy: [0.2, 0.5, 1.0] \n",
            "\n",
            "Accuracy: [0.0, 0.0, 0.1] \n",
            "\n",
            "Accuracy: [0.7, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.0, 0.6, 0.6] \n",
            "\n",
            "Accuracy: [0.5, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.6, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.9, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.8, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.2, 0.4, 0.7] \n",
            "\n",
            "Accuracy: [0.3, 0.5, 0.5] \n",
            "\n",
            "Accuracy: [0.4, 0.7, 1.0] \n",
            "\n",
            "Accuracy: [0.6, 0.9, 1.0] \n",
            "\n",
            "Accuracy: [0.4, 0.9, 1.0] \n",
            "\n",
            "Accuracy: [0.2, 0.3, 0.4] \n",
            "\n",
            "Accuracy: [0.6, 0.9, 0.9] \n",
            "\n",
            "Accuracy: [0.2, 0.4, 0.7] \n",
            "\n",
            "Accuracy: [0.9, 1.0, 1.0] \n",
            "\n",
            "Accuracy: [0.5, 0.8, 1.0] \n",
            "\n",
            "Accuracy: [0.2, 0.4, 0.7] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "lvq = LVQFirstLevel(distfunc)\n",
        "\n",
        "# Quantize database data\n",
        "Xcomp, delta, codes, l_db, X_means_db = lvq.LVQ_first_level(Xin, B1, 'LVQ', False, qdtype)\n",
        "\n",
        "print(\"Original Vectors:\")\n",
        "print(Xin)\n",
        "\n",
        "print(\"\\n8-Bit Quantized Vectors:\")\n",
        "print(codes)\n",
        "\n",
        "print(\"\\nAfter unquantizing from 8-bit format to float:\")\n",
        "print(Xcomp)\n",
        "\n",
        "# Compute reconstruction error\n",
        "recon_err = lvq.recon_error(Xin, Xcomp)\n",
        "\n",
        "# Quantize query data\n",
        "# Never used\n",
        "Xcomp_query, delta_query, codes_query, l_q, X_means_q = lvq.LVQ_first_level(Xquery, B1, 'LVQ', True, qdtype, X_means_db, l_db, delta)\n",
        "\n",
        "accuracies = []\n",
        "accuracies_quant = []\n",
        "for j, query_vector in enumerate (Xquery):\n",
        "  if(j%50==0):\n",
        "    print(\"\\n================ j = \", j)\n",
        "    print(\"\\nAverage Quantized Accuracy for res_factor=\", res_factors, \":  \", np.mean(np.asarray(accuracies_quant), axis=0))\n",
        "  # Compute  distances and sort\n",
        "  pqorig = PriorityQueue()\n",
        "  pqcode = PriorityQueue()\n",
        "  # pqxcomp = PriorityQueue()\n",
        "\n",
        "  for i, x in enumerate(Xin):\n",
        "      original_dist = lvq.distance(x, query_vector)\n",
        "      code_dist = lvq.distance(codes[i], codes_query[j])\n",
        "      # In our final code, we will use the distance between original query vector and unquantized graph vector\n",
        "      # xcomp_dist = lvq.distance(Xcomp[i], query_vector)\n",
        "\n",
        "      pqorig.put((original_dist, i))\n",
        "      pqcode.put((code_dist, i))\n",
        "      # pqxcomp.put((xcomp_dist, i))\n",
        "\n",
        "  # print(\"\\nOriginal TopK Vector Distances:\")\n",
        "  exact_knn = []\n",
        "  exact_dists = []\n",
        "  for _ in range(K):\n",
        "      dist, idx = pqorig.get()\n",
        "      # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "      exact_knn.append(idx)\n",
        "      exact_dists.append(dist)\n",
        "\n",
        "  # print(\"\\nTopK Distances with 8-bit quantized vectors:\")\n",
        "  quantized_knn = []\n",
        "  for _ in range(len(res_factors)):\n",
        "    quantized_knn.append([])\n",
        "\n",
        "  quantized_dists = []\n",
        "  for _ in range(len(res_factors)):\n",
        "    quantized_dists.append([])\n",
        "\n",
        "  for q in range(K*max(res_factors)):\n",
        "      dist, idx = pqcode.get()\n",
        "      # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "      for w in range(len(res_factors)):\n",
        "        if q < K*res_factors[w]:\n",
        "          quantized_knn[w].append(idx)\n",
        "          quantized_dists[w].append(dist)\n",
        "\n",
        "  # approx_knn = []\n",
        "  # # print(\"\\nTopK Distances after unquantizing:\")\n",
        "  # for _ in range(K):\n",
        "  #     dist, idx = pqxcomp.get()\n",
        "  #     # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "  #     approx_knn.append(idx)\n",
        "\n",
        "  # accuracy = lvq.compute_accuracy(exact_knn, approx_knn)\n",
        "  # accuracies.append(accuracy)\n",
        "  # # print(\"\\nAccuracy:\", accuracy)\n",
        "\n",
        "  accuracy_quant = [lvq.compute_accuracy(exact_knn, quantized_knn[i]) for i in range(len(res_factors))]\n",
        "  accuracies_quant.append(accuracy_quant)\n",
        "  # print(exact_knn)\n",
        "  # print(exact_dists)\n",
        "  # print(quantized_knn[0])\n",
        "  # print(quantized_dists[0])\n",
        "  # print(quantized_knn[1])\n",
        "  # print(quantized_dists[1])\n",
        "  # print(quantized_knn[2])\n",
        "  # print(quantized_dists[2])\n",
        "  print(\"Accuracy:\", accuracy_quant, \"\\n\")\n",
        "\n",
        "print(\"\\nAverage Reconstruction Error (RMS):\", recon_err)\n",
        "# print(\"\\nAccuracies :\", accuracies)\n",
        "# print(\"\\nAverage Unquantized Accuracy:\", np.mean(accuracies))\n",
        "print(\"\\nAverage Quantized Accuracy for res_factor=\", res_factors, \":  \", np.mean(np.asarray(accuracies_quant), axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYKMvQLxIyLv"
      },
      "source": [
        "# Per Dimension Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHActxJ3RgA"
      },
      "outputs": [],
      "source": [
        "lvq = LVQFirstLevel(distfunc)\n",
        "\n",
        "# Quantize database data\n",
        "Xcomp, delta, codes, l_db, X_means_db = lvq.LVQ_first_level(Xin, B1, alg, False, qdtype)\n",
        "\n",
        "print(np.min(codes))\n",
        "print(np.max(codes))\n",
        "\n",
        "print(\"Original Vectors:\")\n",
        "print(Xin)\n",
        "\n",
        "print(\"\\n8-Bit Quantized Vectors:\")\n",
        "print(codes)\n",
        "\n",
        "print(\"\\nAfter unquantizing from 8-bit format to float:\")\n",
        "print(Xcomp)\n",
        "\n",
        "# Compute reconstruction error\n",
        "recon_err = lvq.recon_error(Xin, Xcomp)\n",
        "\n",
        "# Quantize query data\n",
        "# Never used\n",
        "Xcomp_query, delta_query, codes_query, l_q, X_means_q = lvq.LVQ_first_level(Xquery, B1, alg, True, qdtype, X_means_db, l_db, delta)\n",
        "\n",
        "accuracies = []\n",
        "accuracies_quant = []\n",
        "for j, query_vector in enumerate (Xquery):\n",
        "  if(j%50==0):\n",
        "    print(\"\\n================ j = \", j)\n",
        "    print(\"\\nAverage Quantized Accuracy for res_factor=\", res_factors, \":  \", np.mean(np.asarray(accuracies_quant), axis=0))\n",
        "  # Compute  distances and sort\n",
        "  pqorig = PriorityQueue()\n",
        "  pqcode = PriorityQueue()\n",
        "  # pqxcomp = PriorityQueue()\n",
        "\n",
        "  for i, x in enumerate(Xin):\n",
        "      original_dist = lvq.distance(x, query_vector)\n",
        "      code_dist = lvq.distance(codes[i], codes_query[j])\n",
        "      # In our final code, we will use the distance between original query vector and unquantized graph vector\n",
        "      # xcomp_dist = lvq.distance(Xcomp[i], query_vector)\n",
        "\n",
        "      pqorig.put((original_dist, i))\n",
        "      pqcode.put((code_dist, i))\n",
        "      # pqxcomp.put((xcomp_dist, i))\n",
        "\n",
        "  # print(\"\\nOriginal TopK Vector Distances:\")\n",
        "  exact_knn = []\n",
        "  exact_dists = []\n",
        "  for _ in range(K):\n",
        "      dist, idx = pqorig.get()\n",
        "      # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "      exact_knn.append(idx)\n",
        "      exact_dists.append(dist)\n",
        "\n",
        "  # print(\"\\nTopK Distances with 8-bit quantized vectors:\")\n",
        "  quantized_knn = []\n",
        "  for _ in range(len(res_factors)):\n",
        "    quantized_knn.append([])\n",
        "\n",
        "  quantized_dists = []\n",
        "  for _ in range(len(res_factors)):\n",
        "    quantized_dists.append([])\n",
        "\n",
        "  for q in range(K*max(res_factors)):\n",
        "      dist, idx = pqcode.get()\n",
        "      # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "      for w in range(len(res_factors)):\n",
        "        if q < K*res_factors[w]:\n",
        "          quantized_knn[w].append(idx)\n",
        "          quantized_dists[w].append(dist)\n",
        "\n",
        "  # approx_knn = []\n",
        "  # # print(\"\\nTopK Distances after unquantizing:\")\n",
        "  # for _ in range(K):\n",
        "  #     dist, idx = pqxcomp.get()\n",
        "  #     # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "  #     approx_knn.append(idx)\n",
        "\n",
        "  # accuracy = lvq.compute_accuracy(exact_knn, approx_knn)\n",
        "  # accuracies.append(accuracy)\n",
        "  # # print(\"\\nAccuracy:\", accuracy)\n",
        "\n",
        "  accuracy_quant = [lvq.compute_accuracy(exact_knn, quantized_knn[i]) for i in range(len(res_factors))]\n",
        "  accuracies_quant.append(accuracy_quant)\n",
        "  print(\"Accuracy:\", accuracy_quant, \"\\n\")\n",
        "\n",
        "print(\"\\nAverage Reconstruction Error (RMS):\", recon_err)\n",
        "# print(\"\\nAccuracies :\", accuracies)\n",
        "# print(\"\\nAverage Unquantized Accuracy:\", np.mean(accuracies))\n",
        "print(\"\\nAverage Quantized Accuracy for res_factor=\", res_factors, \":  \", 100*np.mean(np.asarray(accuracies_quant), axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsFaHr5gFqFa"
      },
      "source": [
        "# Combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSo3j8GqJH7M"
      },
      "outputs": [],
      "source": [
        "# Returns exact_knn: Shape (num_query_vectors, K)\n",
        "def get_exact_topk(Xin, Xquery, lvq):\n",
        "  # Number of dimensions must be the same\n",
        "  assert(np.shape(Xin)[1] == np.shape(Xquery)[1])\n",
        "\n",
        "  num_query_vectors = np.shape(Xquery)[0]\n",
        "\n",
        "  print(\"Populate the priority queue for each query\")\n",
        "  pqorig = []\n",
        "  for _ in range(num_query_vectors):\n",
        "    pqorig.append(PriorityQueue())\n",
        "  for j, query_vector in enumerate (Xquery):\n",
        "    for i, x in enumerate(Xin):\n",
        "      original_dist = lvq.euclidean_distance(x, query_vector)\n",
        "      pqorig[j].put((original_dist, i))\n",
        "\n",
        "  print(\"Populate the topK VIDs for each query\")\n",
        "  exact_knn = []\n",
        "  for _ in range(num_query_vectors):\n",
        "    exact_knn.append([])\n",
        "  for j, query_vector in enumerate (Xquery):\n",
        "    for _ in range(K):\n",
        "      dist, idx = pqorig[j].get()\n",
        "      # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "      print(j, num_query_vectors)\n",
        "      exact_knn[j].append(idx)\n",
        "\n",
        "  return exact_knn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "Uq-kL-_EFo2E",
        "outputId": "23feb03a-4a19-4c1d-ee81-d7a6bd8b1877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Xin shape: (10000, 960)\n",
            "X_means shape: (960,)\n",
            "X shape: (10000, 960)\n",
            "l,u shape: (10000,)\n",
            "delta shape: (10000,)\n",
            "inv_delta shape: (10000,)\n",
            "Xcomp shape: (10000, 960)\n",
            "codes shape: (10000, 960)\n",
            "Xin shape: (1000, 960)\n",
            "X_means shape: (960,)\n",
            "X shape: (1000, 960)\n",
            "l,u shape: (1000,)\n",
            "delta shape: (1000,)\n",
            "inv_delta shape: (1000,)\n",
            "Xcomp shape: (1000, 960)\n",
            "codes shape: (1000, 960)\n",
            "Xin shape: (10000, 960)\n",
            "X_means shape: (960,)\n",
            "X shape: (10000, 960)\n",
            "l,u shape: (960,)\n",
            "delta shape: (960,)\n",
            "inv_delta shape: (960,)\n",
            "Xcomp shape: (10000, 960)\n",
            "codes shape: (10000, 960)\n",
            "Xin shape: (1000, 960)\n",
            "X_means shape: (960,)\n",
            "X shape: (1000, 960)\n",
            "l,u shape: (960,)\n",
            "delta shape: (960,)\n",
            "inv_delta shape: (960,)\n",
            "Xcomp shape: (1000, 960)\n",
            "codes shape: (1000, 960)\n",
            "Populate the priority queue for each query\n",
            "Populate the topK VIDs for each query\n",
            "0 1000\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b4f8baf84033>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_exact\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexact_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;31m# Compute the exact topK for all query vectors:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mexact_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_exact_topk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Accuracies with unquantized stored vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f8022f84b870>\u001b[0m in \u001b[0;36mget_exact_topk\u001b[0;34m(Xin, Xquery, lvq)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;31m# print(f\"Vector ID: {idx}, Distance: {dist}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_query_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mexact_knn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexact_knn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "compute_exact   = True\n",
        "# compute algorithms\n",
        "doThisAlg = [True, True]\n",
        "\n",
        "num_algs = 2 # LVQ, PerDim\n",
        "# Xcomp, delta, codes, l_db, X_means_db, Xcomp_query, delta_query, codes_query, l_q, X_means_q  = []\n",
        "# # Accuracies with unquantized stored vectors\n",
        "# accuracies = []     # LVQ, PerDim\n",
        "# # Accuracies with quantized query vectors\n",
        "# accuracies_quant = []     # LVQ, PerDim\n",
        "# for _ in range(num_algs):\n",
        "#   Xcomp.append([])\n",
        "#   delta.append([])\n",
        "#   codes.append([])\n",
        "#   l_db.append([])\n",
        "#   X_means_db.append([])\n",
        "\n",
        "#   Xcomp_query.append([])\n",
        "#   delta_query.append([])\n",
        "#   codes_query.append([])\n",
        "#   l_q.append([])\n",
        "#   X_means_q.append([])\n",
        "\n",
        "#   accuracies.append([])\n",
        "#   accuracies_quant.append([])\n",
        "\n",
        "lvq = LVQFirstLevel()\n",
        "\n",
        "\n",
        "if(compute_exact or exact_knn.any() == None):\n",
        "  # Compute the exact topK for all query vectors:\n",
        "  exact_knn = get_exact_topk(Xin, Xquery, lvq)\n",
        "\n",
        "\n",
        "for algidx in range(num_algs):\n",
        "  if(doThisAlg[algidx]):\n",
        "    isPerDim = (algidx == 1)\n",
        "\n",
        "    # Quantize database data, LVQ\n",
        "    Xcomp[algidx], delta[algidx], codes[algidx], l_db[algidx], \\\n",
        "      X_means_db[algidx] = \\\n",
        "      lvq.LVQ_first_level(Xin, B1, isPerDim, False)\n",
        "    # Quantize query data, LVQ\n",
        "    Xcomp_query[algidx], delta_query[algidx], codes_query[algidx], l_q[algidx],\\\n",
        "      X_means_q[algidx] = \\\n",
        "      lvq.LVQ_first_level(Xquery[algidx], B1, isPerDim, True, \\\n",
        "        X_means_db[algidx], l_db[algidx], delta[algidx])\n",
        "\n",
        "    # print(\"Original Vectors:\")\n",
        "    # print(Xin)\n",
        "\n",
        "    # print(\"\\n8-Bit Quantized Vectors:\")\n",
        "    # print(codes[algidx])\n",
        "\n",
        "    # print(\"\\nAfter unquantizing from 8-bit format to float:\")\n",
        "    # print(Xcomp[algidx])\n",
        "\n",
        "\n",
        "    # Compute reconstruction error\n",
        "    recon_err = lvq.recon_error(Xin, Xcomp)\n",
        "\n",
        "\n",
        "    for j, query_vector in enumerate (Xquery):\n",
        "      if(j%50==0):\n",
        "        print(\"\\n================ j = \", j)\n",
        "        print(\"\\nAverage Quantized Accuracy for res_factor=\", res_factors, \":  \", np.mean(np.asarray(accuracies_quant), axis=0))\n",
        "      # Compute distances and sort\n",
        "      pqcodes = []     # LVQ, PerDim\n",
        "      pqxcomps = []     # LVQ, PerDim\n",
        "      for _ in range(num_algs):\n",
        "        pqcodes.append(PriorityQueue())\n",
        "        pqxcomps.append(PriorityQueue())\n",
        "\n",
        "      for i, x in enumerate(Xin):\n",
        "        ## LVQ\n",
        "        code_dist = lvq.distance(codes[i], codes_query[j])\n",
        "        # xcomp_dist = lvq.euclidean_distance(Xcomp[i], query_vector)\n",
        "\n",
        "        pqcodes[0].put((code_dist, i))\n",
        "        # pqxcomp[0].put((xcomp_dist, i))\n",
        "\n",
        "        ## PerDim\n",
        "        code_dist = lvq.distance(codes_2[i], codes_query[j])\n",
        "        # xcomp_dist = lvq.euclidean_distance(Xcomp_2[i], query_vector)\n",
        "\n",
        "        pqcodes[1].put((code_dist, i))\n",
        "        # pqxcomp[1].put((xcomp_dist, i))\n",
        "\n",
        "      ## Quantized Accuracy\n",
        "      # print(\"\\nTopK Distances with 8-bit quantized vectors:\")\n",
        "      for e in num_algs: # LVQ, PerDim\n",
        "        quantized_knn = []\n",
        "        for _ in range(len(res_factors)):\n",
        "          quantized_knn.append([])\n",
        "\n",
        "        for q in range(K*max(res_factors)):\n",
        "            dist, idx = pqcodes[e].get()\n",
        "            # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "            for w in range(len(res_factors)):\n",
        "              if q < K*res_factors[w]:\n",
        "                quantized_knn[w].append(idx)\n",
        "\n",
        "        accuracy_quant = [lvq.compute_accuracy(exact_knn[j], quantized_knn[w]) for w in range(len(res_factors))]\n",
        "        accuracies_quant[e].append(accuracy_quant)\n",
        "        print(\"\\nAccuracy:\", accuracy_quant)\n",
        "\n",
        "      ## Unquantized Accuracy\n",
        "      # for e in num_algs: # LVQ, PerDim\n",
        "        # approx_knn = []\n",
        "        # # print(\"\\nTopK Distances after unquantizing:\")\n",
        "        # for _ in range(K):\n",
        "        #     dist, idx = pqxcomps[e].get()\n",
        "        #     # print(f\"Vector ID: {idx}, Distance: {dist}\")\n",
        "        #     approx_knn.append(idx)\n",
        "\n",
        "        # accuracy = lvq.compute_accuracy(exact_knn[j], approx_knn)\n",
        "        # accuracies[e].append(accuracy)\n",
        "        # print(\"\\nAccuracy:\", accuracy)\n",
        "\n",
        "print(\"\\nAverage Reconstruction Error (RMS):\", recon_err)\n",
        "for e in num_algs: # LVQ, PerDim\n",
        "  # print(\"\\nAccuracies :\", accuracies[e])\n",
        "  # print(\"\\nAverage Unquantized Accuracy:\", np.mean(accuracies[e]))\n",
        "  print(\"\\nAverage Quantized Accuracy for algorithm\", e, \" for res_factor=\", res_factors, \":  \", np.mean(np.asarray(accuracies_quant[e]), axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtSird50mfOI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nrbmBuhrIVlU",
        "NsFaHr5gFqFa"
      ],
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}